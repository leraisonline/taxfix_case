# Objective 3: Propose a production approach
- Be ready to discuss a high-level overview of a production-ready version of the same pipeline
- What changes would you make to your code?
- Which solutions would you propose?

## Repository:

### Suggestion:
- Use poetry for version control instead of requirements.txt

#### Why?
- Poetry can handle multiple envs (for exp, multiple prod or prod+dev, etc.)
- Poetry has the poetry.lock file with all the dependency hashes: we will know everything about our dependencies and how they sync with prod in one place

### Suggestion:
- Better error handling and logging

#### Why?
- In this code we have some simple logging. But it's better to make logging levels deeper for faster debugging

### Suggestion:
- Config in env vars or another proper config system

#### Why?
- Security
- Now we need to commit something to the code to change values. It can be difficult to maintain in prod

### Suggestion:
- Use async where it makes sense

#### Why?
- We can speed up (if we need) some operations - postgres call (asyncpg) or api call/calls

### Suggestion:
- Stricter data checks (Pydantic or attrs libs)

#### Why?
- We are trying to handle possible errors early
- We can generate the documentation from our checks/classes, reuse this approach for other data and scale it easier

### Suggestion:
- More tests (unit, integration, e2e)

#### Why?
- Now we have only simple unit tests. But we don't know whether our approach works in Airflow env/Other prod env
- We want to be sure that our approach will work with our db/dbs

### Suggestion:
- Set up CI/CD

#### Why?
- CI - for the quality validation and dependencies monitoring
- CD - for the automotive rule-based deployment

### Suggestion:
- Autogenerated Docs

#### Why?
- It helps to maintain code (and understand it)
- Better onboarding

### Suggestion:
- Custom dependency maintenance

#### Why?
For example, we have 100 scripts/DAGs that read the same data from Postgres, and we want to maintain all the calls and library dependencies from one place.
We can create a library (for example postgres_utils) where we'll do it. If we need to change DB, connection, add some features or upgrade library versions, we can do it in one place.


## Airflow or other orchestration env:

### Suggestion:
- Use small batches/intervals
#### Why?
- Workers in Airflow aren't strong (regarding CPU and data amount). We can use less resources if we divide data to smaller batches.
- We can fail faster and fix these fails faster

### Suggestion:
- Custom storage instead of XCom for big data
#### Why?
- XCOM is quite limited and buggy. We can easily implement a saving method for our cloud data in parquet.
- In this scenario we will share string hash keys between jobs.

### Suggestion:
- Retry mechanism for dags/tasks
#### Why?
- We can't expect that every system will be 100% available
- Sometimes a simple retry fixes the problem. On the airflow level we can easily handle these things as params

### Suggestion:
- Use Airflow's built-in monitoring (for exp, in GCP Composer) or create your own monitoring system
#### Why?
- We can monitor historical incidents, resources consumption

### Data suggestions:
- Validate data and enforce schemas (garbage in, garbage out)
- Partitioning for speed
- Proper indexing
- Data lineage
- Data cataloging
- Data tracing
- Query insight mechanisms

### Security:
- IAM for access control
- Service account control

### Disaster recovery:
- Backups/snapshots (we can recover or create a new env faster)
- Multi-region setup (if we need it and the budget allows)

### Other suggestions (shortly):
- Spark or Kafka for heavy lifting (only if we need it)
- PostgreSQL/other DB instead of SQLite
- Separate reporting service (with Tableau, for exp)
- Caching (Redis) for frequent reports
- Set up alerts for crucial business things
- Terraform for infrastructure management
- Auto-scaling (downscaling and upscaling) for the orchestration env. It can save money and resources consumption

### High level documentation
- API docs (openapi yaml helps a lot)
- Jupyter Notebooks with examples
- Architecture diagrams for pipelines/solutions for better understanding
- Data model docs
- User guides (for exp, in notion/another documentation system)
